{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ba2fv357L2W",
        "outputId": "e5af8a4c-b200-40dc-c8e3-b2a4af46ba4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "OASIS Ultimate Pipeline v4.3 - IMPROVED (Training Issues Fixed)\n",
            "================================================================================\n",
            "âœ“ Device: cuda\n",
            "  GPU: NVIDIA L4\n",
            "  Memory: 23.80 GB\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "âœ“ Initially found 840 files | CN=754, MCI=86\n",
            "ðŸ“Š Original CN samples: 754\n",
            "ðŸŽ¯ Randomly sampling 440 CN cases...\n",
            "âœ… Balanced dataset: CN=440, MCI=86\n",
            "ðŸ“Š Final class imbalance ratio: 5.12:1 (CN:MCI)\n",
            "âœ“ Training class distribution: CN=308, MCI=60\n",
            "âœ“ Class weights: CN=0.597, MCI=5.520\n",
            "\n",
            "============================================================\n",
            "STARTING TRAINING WITH IMPROVED MONITORING\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/60:\n",
            "  Train - Loss: 0.3798, Acc: 0.523, Bal: 0.517\n",
            "  Val   - Loss: 0.2424, Acc: 0.810, Bal: 0.670\n",
            "  Val   - Precision: 0.816, Recall: 0.810, F1: 0.813\n",
            "  Threshold: 0.491, LR: 0.000012\n",
            "  âœ… New best balanced accuracy: 0.670\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2/60:\n",
            "  Train - Loss: 0.3632, Acc: 0.528, Bal: 0.527\n",
            "  Val   - Loss: 0.2355, Acc: 0.380, Bal: 0.598\n",
            "  Val   - Precision: 0.824, Recall: 0.380, F1: 0.408\n",
            "  Threshold: 0.425, LR: 0.000012\n",
            "  â³ No improvement. Patience: 1/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3/60:\n",
            "  Train - Loss: 0.3477, Acc: 0.574, Bal: 0.571\n",
            "  Val   - Loss: 0.2212, Acc: 0.608, Bal: 0.734\n",
            "  Val   - Precision: 0.860, Recall: 0.608, F1: 0.656\n",
            "  Threshold: 0.425, LR: 0.000012\n",
            "  âœ… New best balanced accuracy: 0.734\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4/60:\n",
            "  Train - Loss: 0.3698, Acc: 0.483, Bal: 0.475\n",
            "  Val   - Loss: 0.2163, Acc: 0.620, Bal: 0.742\n",
            "  Val   - Precision: 0.862, Recall: 0.620, F1: 0.668\n",
            "  Threshold: 0.411, LR: 0.000012\n",
            "  âœ… New best balanced accuracy: 0.742\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5/60:\n",
            "  Train - Loss: 0.3687, Acc: 0.548, Bal: 0.558\n",
            "  Val   - Loss: 0.2117, Acc: 0.709, Bal: 0.764\n",
            "  Val   - Precision: 0.856, Recall: 0.709, F1: 0.746\n",
            "  Threshold: 0.410, LR: 0.000012\n",
            "  âœ… New best balanced accuracy: 0.764\n",
            "âœ“ Unfroze all layers at epoch 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6/60:\n",
            "  Train - Loss: 0.3952, Acc: 0.483, Bal: 0.479\n",
            "  Val   - Loss: 0.2217, Acc: 0.696, Bal: 0.756\n",
            "  Val   - Precision: 0.854, Recall: 0.696, F1: 0.735\n",
            "  Threshold: 0.411, LR: 0.000012\n",
            "  â³ No improvement. Patience: 1/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7/60:\n",
            "  Train - Loss: 0.3691, Acc: 0.514, Bal: 0.506\n",
            "  Val   - Loss: 0.2183, Acc: 0.671, Bal: 0.741\n",
            "  Val   - Precision: 0.849, Recall: 0.671, F1: 0.713\n",
            "  Threshold: 0.423, LR: 0.000012\n",
            "  â³ No improvement. Patience: 2/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8/60:\n",
            "  Train - Loss: 0.3557, Acc: 0.517, Bal: 0.511\n",
            "  Val   - Loss: 0.2139, Acc: 0.608, Bal: 0.765\n",
            "  Val   - Precision: 0.884, Recall: 0.608, F1: 0.654\n",
            "  Threshold: 0.393, LR: 0.000012\n",
            "  âœ… New best balanced accuracy: 0.765\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9/60:\n",
            "  Train - Loss: 0.3517, Acc: 0.528, Bal: 0.528\n",
            "  Val   - Loss: 0.2116, Acc: 0.734, Bal: 0.748\n",
            "  Val   - Precision: 0.845, Recall: 0.734, F1: 0.766\n",
            "  Threshold: 0.426, LR: 0.000012\n",
            "  â³ No improvement. Patience: 1/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10/60:\n",
            "  Train - Loss: 0.3481, Acc: 0.543, Bal: 0.540\n",
            "  Val   - Loss: 0.2152, Acc: 0.684, Bal: 0.811\n",
            "  Val   - Precision: 0.892, Recall: 0.684, F1: 0.724\n",
            "  Threshold: 0.427, LR: 0.000012\n",
            "  âœ… New best balanced accuracy: 0.811\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 11/60:\n",
            "  Train - Loss: 0.3420, Acc: 0.526, Bal: 0.526\n",
            "  Val   - Loss: 0.2069, Acc: 0.848, Bal: 0.816\n",
            "  Val   - Precision: 0.880, Recall: 0.848, F1: 0.859\n",
            "  Threshold: 0.472, LR: 0.000012\n",
            "  âœ… New best balanced accuracy: 0.816\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 12/60:\n",
            "  Train - Loss: 0.3508, Acc: 0.517, Bal: 0.507\n",
            "  Val   - Loss: 0.2115, Acc: 0.823, Bal: 0.740\n",
            "  Val   - Precision: 0.846, Recall: 0.823, F1: 0.832\n",
            "  Threshold: 0.477, LR: 0.000012\n",
            "  â³ No improvement. Patience: 1/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 13/60:\n",
            "  Train - Loss: 0.3571, Acc: 0.514, Bal: 0.512\n",
            "  Val   - Loss: 0.2073, Acc: 0.734, Bal: 0.810\n",
            "  Val   - Precision: 0.879, Recall: 0.734, F1: 0.768\n",
            "  Threshold: 0.439, LR: 0.000012\n",
            "  â³ No improvement. Patience: 2/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 14/60:\n",
            "  Train - Loss: 0.3284, Acc: 0.588, Bal: 0.575\n",
            "  Val   - Loss: 0.2108, Acc: 0.759, Bal: 0.825\n",
            "  Val   - Precision: 0.884, Recall: 0.759, F1: 0.789\n",
            "  Threshold: 0.453, LR: 0.000012\n",
            "  âœ… New best balanced accuracy: 0.825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 15/60:\n",
            "  Train - Loss: 0.3457, Acc: 0.540, Bal: 0.533\n",
            "  Val   - Loss: 0.2035, Acc: 0.823, Bal: 0.863\n",
            "  Val   - Precision: 0.899, Recall: 0.823, F1: 0.842\n",
            "  Threshold: 0.435, LR: 0.000012\n",
            "  âœ… New best balanced accuracy: 0.863\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 16/60:\n",
            "  Train - Loss: 0.3455, Acc: 0.619, Bal: 0.613\n",
            "  Val   - Loss: 0.2097, Acc: 0.709, Bal: 0.826\n",
            "  Val   - Precision: 0.895, Recall: 0.709, F1: 0.746\n",
            "  Threshold: 0.412, LR: 0.000012\n",
            "  â³ No improvement. Patience: 1/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 17/60:\n",
            "  Train - Loss: 0.3197, Acc: 0.582, Bal: 0.579\n",
            "  Val   - Loss: 0.2089, Acc: 0.595, Bal: 0.758\n",
            "  Val   - Precision: 0.883, Recall: 0.595, F1: 0.642\n",
            "  Threshold: 0.380, LR: 0.000012\n",
            "  â³ No improvement. Patience: 2/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 18/60:\n",
            "  Train - Loss: 0.3544, Acc: 0.514, Bal: 0.502\n",
            "  Val   - Loss: 0.1993, Acc: 0.810, Bal: 0.825\n",
            "  Val   - Precision: 0.880, Recall: 0.810, F1: 0.830\n",
            "  Threshold: 0.452, LR: 0.000012\n",
            "  â³ No improvement. Patience: 3/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 19/60:\n",
            "  Train - Loss: 0.3313, Acc: 0.562, Bal: 0.557\n",
            "  Val   - Loss: 0.2039, Acc: 0.848, Bal: 0.786\n",
            "  Val   - Precision: 0.869, Recall: 0.848, F1: 0.856\n",
            "  Threshold: 0.451, LR: 0.000012\n",
            "  â³ No improvement. Patience: 4/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 20/60:\n",
            "  Train - Loss: 0.3202, Acc: 0.585, Bal: 0.582\n",
            "  Val   - Loss: 0.1903, Acc: 0.747, Bal: 0.818\n",
            "  Val   - Precision: 0.882, Recall: 0.747, F1: 0.779\n",
            "  Threshold: 0.387, LR: 0.000012\n",
            "  â³ No improvement. Patience: 5/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 21/60:\n",
            "  Train - Loss: 0.3180, Acc: 0.597, Bal: 0.593\n",
            "  Val   - Loss: 0.1899, Acc: 0.734, Bal: 0.841\n",
            "  Val   - Precision: 0.898, Recall: 0.734, F1: 0.768\n",
            "  Threshold: 0.386, LR: 0.000012\n",
            "  â³ No improvement. Patience: 6/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 22/60:\n",
            "  Train - Loss: 0.3432, Acc: 0.560, Bal: 0.550\n",
            "  Val   - Loss: 0.1987, Acc: 0.671, Bal: 0.803\n",
            "  Val   - Precision: 0.890, Recall: 0.671, F1: 0.713\n",
            "  Threshold: 0.379, LR: 0.000012\n",
            "  â³ No improvement. Patience: 7/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 23/60:\n",
            "  Train - Loss: 0.3212, Acc: 0.560, Bal: 0.555\n",
            "  Val   - Loss: 0.1991, Acc: 0.835, Bal: 0.809\n",
            "  Val   - Precision: 0.875, Recall: 0.835, F1: 0.848\n",
            "  Threshold: 0.497, LR: 0.000012\n",
            "  â³ No improvement. Patience: 8/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 24/60:\n",
            "  Train - Loss: 0.3480, Acc: 0.594, Bal: 0.585\n",
            "  Val   - Loss: 0.2010, Acc: 0.924, Bal: 0.831\n",
            "  Val   - Precision: 0.921, Recall: 0.924, F1: 0.921\n",
            "  Threshold: 0.560, LR: 0.000012\n",
            "  â³ No improvement. Patience: 9/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 25/60:\n",
            "  Train - Loss: 0.3223, Acc: 0.634, Bal: 0.636\n",
            "  Val   - Loss: 0.2101, Acc: 0.810, Bal: 0.794\n",
            "  Val   - Precision: 0.866, Recall: 0.810, F1: 0.828\n",
            "  Threshold: 0.503, LR: 0.000012\n",
            "  â³ No improvement. Patience: 10/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 26/60:\n",
            "  Train - Loss: 0.3280, Acc: 0.577, Bal: 0.568\n",
            "  Val   - Loss: 0.2041, Acc: 0.658, Bal: 0.765\n",
            "  Val   - Precision: 0.867, Recall: 0.658, F1: 0.702\n",
            "  Threshold: 0.417, LR: 0.000012\n",
            "  â³ No improvement. Patience: 11/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 27/60:\n",
            "  Train - Loss: 0.3386, Acc: 0.602, Bal: 0.594\n",
            "  Val   - Loss: 0.1972, Acc: 0.747, Bal: 0.756\n",
            "  Val   - Precision: 0.848, Recall: 0.747, F1: 0.776\n",
            "  Threshold: 0.441, LR: 0.000012\n",
            "  â³ No improvement. Patience: 12/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 28/60:\n",
            "  Train - Loss: 0.3419, Acc: 0.577, Bal: 0.565\n",
            "  Val   - Loss: 0.2010, Acc: 0.633, Bal: 0.780\n",
            "  Val   - Precision: 0.886, Recall: 0.633, F1: 0.678\n",
            "  Threshold: 0.353, LR: 0.000012\n",
            "  â³ No improvement. Patience: 13/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 29/60:\n",
            "  Train - Loss: 0.3202, Acc: 0.582, Bal: 0.573\n",
            "  Val   - Loss: 0.1970, Acc: 0.722, Bal: 0.772\n",
            "  Val   - Precision: 0.859, Recall: 0.722, F1: 0.756\n",
            "  Threshold: 0.436, LR: 0.000012\n",
            "  â³ No improvement. Patience: 14/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 30/60:\n",
            "  Train - Loss: 0.3219, Acc: 0.585, Bal: 0.589\n",
            "  Val   - Loss: 0.1992, Acc: 0.747, Bal: 0.818\n",
            "  Val   - Precision: 0.882, Recall: 0.747, F1: 0.779\n",
            "  Threshold: 0.437, LR: 0.000012\n",
            "  â³ No improvement. Patience: 15/15\n",
            "  ðŸ›‘ Early stopping at epoch 30\n",
            "\n",
            "============================================================\n",
            "LOADING BEST MODEL FOR FINAL EVALUATION\n",
            "============================================================\n",
            "âœ“ Loaded best model from epoch 15\n",
            "âœ“ Best validation balanced accuracy: 0.863\n",
            "\n",
            "============================================================\n",
            "FINAL TEST EVALUATION\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸŽ¯ FINAL TEST RESULTS:\n",
            "  Accuracy: 0.873 (87.3%)\n",
            "  Balanced Accuracy: 0.801 (80.1%)\n",
            "  Precision: 0.882\n",
            "  Recall: 0.873\n",
            "  F1-Score: 0.877\n",
            "  Optimal Threshold: 0.464\n",
            "\n",
            "ðŸ“Š MEDICAL METRICS:\n",
            "  Sensitivity (MCI Detection): 0.692 (69.2%)\n",
            "  Specificity (CN Detection): 0.909 (90.9%)\n",
            "  Positive Predictive Value: 0.600 (60.0%)\n",
            "  Negative Predictive Value: 0.938 (93.8%)\n",
            "\n",
            "ðŸ“ˆ CONFUSION MATRIX:\n",
            "     Predicted\n",
            "      CN  MCI\n",
            " CN   60   6\n",
            "MCI    4   9\n",
            "\n",
            "ðŸŽ¯ AUC-ROC: 0.760\n",
            "\n",
            "âœ… TRAINING COMPLETED SUCCESSFULLY!\n",
            "ðŸ“„ Best model saved as 'best.pth'\n"
          ]
        }
      ],
      "source": [
        "# ====================\n",
        "# OASIS CN vs MCI\n",
        "# ====================\n",
        "import os, random, warnings\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Vision\n",
        "import torchvision.transforms as T\n",
        "import torchvision.models as models\n",
        "\n",
        "# Imaging\n",
        "import nibabel as nib\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from skimage import exposure\n",
        "\n",
        "# Metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, balanced_accuracy_score, roc_curve, auc\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Colab Drive\n",
        "from google.colab import drive\n",
        "\n",
        "# ==================== CONFIGURATION ====================\n",
        "class Config:\n",
        "    DATA_ROOT = \"/content/drive/MyDrive/OASIS3_Preprocessed_Data\"\n",
        "    IMAGE_SIZE = 256\n",
        "    BATCH_SIZE = 32\n",
        "    NUM_EPOCHS = 60\n",
        "    LEARNING_RATE = 3e-5\n",
        "    WEIGHT_DECAY = 1e-4\n",
        "\n",
        "    # 2.5D multi-slice extraction\n",
        "    USE_25D = True\n",
        "    SLAB_OFFSETS = [-12, -6, 0, 6, 12]  # anchors\n",
        "    SLAB_SAMPLES = 3  # random 3 each epoch\n",
        "\n",
        "    # Class balancing - calculated dynamically\n",
        "    USE_DYNAMIC_WEIGHTS = True\n",
        "    BASE_MCI_WEIGHT = 1.8  # Base multiplier for MCI class\n",
        "\n",
        "    # IMPROVED Loss function\n",
        "    USE_FOCAL_LOSS = True\n",
        "    FOCAL_ALPHA = 0.85  # Slightly favor minority class\n",
        "    FOCAL_GAMMA = 1.5   # Reduced gamma for better convergence\n",
        "    LABEL_SMOOTHING = 0.05  # Reduced for better training\n",
        "\n",
        "    # Augmentation\n",
        "    USE_MIXUP = True\n",
        "    MIXUP_ALPHA = 0.2\n",
        "    USE_CUTMIX = True\n",
        "    CUTMIX_PROB = 0.2\n",
        "\n",
        "    # Threshold optimization\n",
        "    OPTIMIZE_THRESHOLD = True\n",
        "\n",
        "    # Model\n",
        "    MODEL_TYPE = 'convnext'\n",
        "\n",
        "    # IMPROVED Training\n",
        "    PATIENCE = 15\n",
        "    WARMUP_EPOCHS = 5  # Gradual learning rate warmup\n",
        "    MIN_DELTA = 0.001  # Minimum improvement for early stopping\n",
        "\n",
        "    CLASSES = ['CN', 'MCI']\n",
        "    NUM_CLASSES = 2\n",
        "\n",
        "cfg = Config()\n",
        "\n",
        "# ==================== SETUP ====================\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "set_seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"=\"*80)\n",
        "print(\"OASIS CN vs MCI Pipeline\")\n",
        "print(\"=\"*80)\n",
        "print(f\"âœ“ Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# ==================== PREPROCESSING ====================\n",
        "class VolumePreprocessor:\n",
        "    @staticmethod\n",
        "    def extract_25d_slabs(volume):\n",
        "        if volume.ndim == 4:\n",
        "            volume = volume[..., 0]\n",
        "        if volume.ndim != 3:\n",
        "            return None\n",
        "        h, w, d = volume.shape\n",
        "        center = d // 2\n",
        "        offsets = random.sample(cfg.SLAB_OFFSETS, cfg.SLAB_SAMPLES)\n",
        "        slabs = []\n",
        "        for off in offsets:\n",
        "            idx = int(np.clip(center + off, 0, d-1))\n",
        "            slice_img = volume[:, :, idx]\n",
        "            # robust per-slice normalization\n",
        "            p1, p99 = np.percentile(slice_img, [1, 99])\n",
        "            slice_img = np.clip(slice_img, p1, p99)\n",
        "            slice_img = (slice_img - p1) / (p99 - p1 + 1e-8)\n",
        "            slice_img = exposure.equalize_adapthist(slice_img, clip_limit=0.03)\n",
        "            slabs.append(slice_img)\n",
        "        rgb = np.stack(slabs, axis=-1)\n",
        "        rgb = (rgb * 255).astype(np.uint8)\n",
        "        return rgb\n",
        "\n",
        "# ==================== DATASET ====================\n",
        "class OASISDataset(Dataset):\n",
        "    def __init__(self, files, labels, mode='train'):\n",
        "        self.files = files\n",
        "        self.labels = labels\n",
        "        self.mode = mode\n",
        "        self.preprocessor = VolumePreprocessor()\n",
        "\n",
        "        if mode == 'train':\n",
        "            self.transform = T.Compose([\n",
        "                T.Resize((cfg.IMAGE_SIZE+32, cfg.IMAGE_SIZE+32)),\n",
        "                T.RandomCrop(cfg.IMAGE_SIZE),\n",
        "                T.RandomHorizontalFlip(p=0.5),\n",
        "                T.RandomRotation(10),\n",
        "                T.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "                T.ToTensor(),\n",
        "                T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "            ])\n",
        "        else:\n",
        "            self.transform = T.Compose([\n",
        "                T.Resize((cfg.IMAGE_SIZE, cfg.IMAGE_SIZE)),\n",
        "                T.ToTensor(),\n",
        "                T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "            ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.files[idx]\n",
        "        label = int(self.labels[idx])\n",
        "\n",
        "        try:\n",
        "            volume = nib.load(path).get_fdata()\n",
        "            if cfg.USE_25D:\n",
        "                rgb = self.preprocessor.extract_25d_slabs(volume)\n",
        "            else:\n",
        "                slice_img = volume[:,:,volume.shape[2]//2]\n",
        "                p1, p99 = np.percentile(slice_img, [1, 99])\n",
        "                slice_img = np.clip(slice_img, p1, p99)\n",
        "                slice_img = (slice_img - p1) / (p99 - p1 + 1e-8)\n",
        "                slice_img = exposure.equalize_adapthist(slice_img, clip_limit=0.03)\n",
        "                rgb = np.stack([slice_img]*3,axis=-1)\n",
        "                rgb = (rgb*255).astype(np.uint8)\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Error loading {path}: {e}\")\n",
        "            # rare I/O hiccup -> return a small noise image to keep batch rolling\n",
        "            rgb = np.random.randint(0,255,(cfg.IMAGE_SIZE,cfg.IMAGE_SIZE,3),dtype=np.uint8)\n",
        "\n",
        "        pil_img = Image.fromarray(rgb)\n",
        "        img_tensor = self.transform(pil_img)\n",
        "        return img_tensor, label\n",
        "\n",
        "# ==================== LOSS FUNCTIONS ====================\n",
        "class ImprovedFocalLoss(nn.Module):\n",
        "    \"\"\"Focal Loss with proper tensor dimensions\"\"\"\n",
        "    def __init__(self, alpha=0.75, gamma=2.0, smoothing=0.05):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.smoothing = smoothing\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        # Apply label smoothing\n",
        "        n_classes = inputs.size(-1)\n",
        "        onehot = F.one_hot(targets, n_classes).to(dtype=inputs.dtype)\n",
        "        onehot = onehot * (1 - self.smoothing) + self.smoothing / n_classes\n",
        "\n",
        "        # Compute cross entropy loss\n",
        "        log_pt = F.log_softmax(inputs, dim=1)\n",
        "        ce_loss = -(onehot * log_pt).sum(dim=1)\n",
        "\n",
        "        # Compute pt for focal weight\n",
        "        pt = torch.exp(-ce_loss)\n",
        "\n",
        "        # Focal weight\n",
        "        focal_weight = (1 - pt) ** self.gamma  # Shape: (batch_size,)\n",
        "\n",
        "        # Alpha weighting (favor minority class)\n",
        "        alpha_weight = torch.where(targets == 1, self.alpha, 1 - self.alpha)  # Shape: (batch_size,)\n",
        "\n",
        "        # Combine losses - all tensors now have shape (batch_size,)\n",
        "        loss = alpha_weight * focal_weight * ce_loss\n",
        "\n",
        "        return loss.mean()\n",
        "\n",
        "class CombinedLoss(nn.Module):\n",
        "    \"\"\"Combine Focal Loss with standard CrossEntropy for stability\"\"\"\n",
        "    def __init__(self, focal_weight=0.7, ce_weight=0.3):\n",
        "        super().__init__()\n",
        "        self.focal_loss = ImprovedFocalLoss(alpha=cfg.FOCAL_ALPHA, gamma=cfg.FOCAL_GAMMA,\n",
        "                                          smoothing=cfg.LABEL_SMOOTHING)\n",
        "        self.ce_loss = nn.CrossEntropyLoss(label_smoothing=cfg.LABEL_SMOOTHING)\n",
        "        self.focal_weight = focal_weight\n",
        "        self.ce_weight = ce_weight\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        focal = self.focal_loss(inputs, targets)\n",
        "        ce = self.ce_loss(inputs, targets)\n",
        "        return self.focal_weight * focal + self.ce_weight * ce\n",
        "\n",
        "# ==================== MODEL ====================\n",
        "def create_model(model_type='convnext'):\n",
        "    if model_type == 'convnext':\n",
        "        try:\n",
        "            weights = models.ConvNeXt_Tiny_Weights.IMAGENET1K_V1\n",
        "            model = models.convnext_tiny(weights=weights)\n",
        "        except Exception:\n",
        "            model = models.convnext_tiny(pretrained=True)\n",
        "        num_features = model.classifier[2].in_features\n",
        "        # Classifier with better regularization\n",
        "        model.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "            nn.Flatten(1),\n",
        "            nn.LayerNorm(num_features),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(num_features, 512),\n",
        "            nn.GELU(),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.GELU(),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, cfg.NUM_CLASSES)\n",
        "        )\n",
        "    elif model_type == 'efficientnet_v2':\n",
        "        try:\n",
        "            weights = models.EfficientNet_V2_S_Weights.IMAGENET1K_V1\n",
        "            model = models.efficientnet_v2_s(weights=weights)\n",
        "        except Exception:\n",
        "            model = models.efficientnet_v2_s(pretrained=True)\n",
        "        num_features = model.classifier[1].in_features\n",
        "        model.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(num_features, 512), nn.SiLU(),\n",
        "            nn.BatchNorm1d(512), nn.Dropout(0.3),\n",
        "            nn.Linear(512, 256), nn.SiLU(),\n",
        "            nn.BatchNorm1d(256), nn.Dropout(0.2),\n",
        "            nn.Linear(256, cfg.NUM_CLASSES)\n",
        "        )\n",
        "    return model.to(device)\n",
        "\n",
        "# ==================== TRAINING UTILS ====================\n",
        "def mixup_data(x,y,alpha=0.2):\n",
        "    if alpha<=0: return x,y,y,1.0\n",
        "    lam = np.random.beta(alpha,alpha)\n",
        "    index = torch.randperm(x.size(0),device=x.device)\n",
        "    mixed_x = lam*x+(1-lam)*x[index]\n",
        "    return mixed_x, y, y[index], lam\n",
        "\n",
        "def cutmix_data(x,y,prob=0.2):\n",
        "    if random.random()>=prob: return x,y,y,1.0\n",
        "    lam = np.random.beta(1.0,1.0)\n",
        "    B,C,W,H=x.size()\n",
        "    index = torch.randperm(B,device=x.device)\n",
        "    cut_rat=np.sqrt(1.-lam)\n",
        "    cut_w=int(W*cut_rat); cut_h=int(H*cut_rat)\n",
        "    cx=np.random.randint(W); cy=np.random.randint(H)\n",
        "    bbx1=np.clip(cx-cut_w//2,0,W); bby1=np.clip(cy-cut_h//2,0,H)\n",
        "    bbx2=np.clip(cx+cut_w//2,0,W); bby2=np.clip(cy+cut_h//2,0,H)\n",
        "    x[:,:,bbx1:bbx2,bby1:bby2]=x[index,:,bbx1:bbx2,bby1:bby2]\n",
        "    lam=1-((bbx2-bbx1)*(bby2-bby1)/(W*H))\n",
        "    return x,y,y[index],lam\n",
        "\n",
        "# ==================== TRAIN / EVAL ====================\n",
        "def train_epoch(model, loader, criterion, optimizer, epoch):\n",
        "    model.train()\n",
        "    total_loss, all_preds, all_labels = 0, [], []\n",
        "\n",
        "    # Progress bar\n",
        "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
        "\n",
        "    for batch_idx, (inputs, labels) in enumerate(pbar):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Apply augmentations with lower probability in early epochs\n",
        "        aug_prob = min(0.7, 0.3 + (epoch / 10) * 0.4)\n",
        "        r = random.random()\n",
        "\n",
        "        if cfg.USE_MIXUP and r < aug_prob * 0.5:\n",
        "            inputs, y_a, y_b, lam = mixup_data(inputs, labels, alpha=cfg.MIXUP_ALPHA)\n",
        "            outputs = model(inputs)\n",
        "            loss = lam * criterion(outputs, y_a) + (1 - lam) * criterion(outputs, y_b)\n",
        "        elif cfg.USE_CUTMIX and r < aug_prob * 0.7:\n",
        "            inputs, y_a, y_b, lam = cutmix_data(inputs, labels, prob=cfg.CUTMIX_PROB)\n",
        "            outputs = model(inputs)\n",
        "            loss = lam * criterion(outputs, y_a) + (1 - lam) * criterion(outputs, y_b)\n",
        "        else:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping for stability\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        all_preds.extend(outputs.argmax(1).detach().cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Update progress bar\n",
        "        if batch_idx % 10 == 0:\n",
        "            pbar.set_postfix({\n",
        "                'Loss': f'{loss.item():.3f}',\n",
        "                'Acc': f'{accuracy_score(all_labels, all_preds):.3f}'\n",
        "            })\n",
        "\n",
        "    train_acc = accuracy_score(all_labels, all_preds)\n",
        "    train_bal = balanced_accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    return total_loss / len(loader), train_acc, train_bal\n",
        "\n",
        "def evaluate(model, loader, criterion, return_probs=False):\n",
        "    model.eval()\n",
        "    all_probs, all_labels = [], []\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            probs = F.softmax(outputs, dim=1)\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    all_probs = np.array(all_probs)\n",
        "    all_labels = np.array(all_labels)\n",
        "    mci_probs = all_probs[:, 1]\n",
        "\n",
        "    # Optimize threshold for balanced accuracy\n",
        "    if cfg.OPTIMIZE_THRESHOLD and len(np.unique(all_labels)) > 1:\n",
        "        fpr, tpr, thresholds = roc_curve(all_labels, mci_probs)\n",
        "        if len(thresholds) > 0:\n",
        "            # Find threshold that maximizes balanced accuracy\n",
        "            balanced_acc_scores = []\n",
        "            for thr in thresholds:\n",
        "                preds = (mci_probs >= thr).astype(int)\n",
        "                bal_acc = balanced_accuracy_score(all_labels, preds)\n",
        "                balanced_acc_scores.append(bal_acc)\n",
        "\n",
        "            best_idx = np.argmax(balanced_acc_scores)\n",
        "            thr_opt = thresholds[best_idx]\n",
        "            preds = (mci_probs >= thr_opt).astype(int)\n",
        "        else:\n",
        "            thr_opt = 0.5\n",
        "            preds = all_probs.argmax(1)\n",
        "    else:\n",
        "        thr_opt = 0.5\n",
        "        preds = all_probs.argmax(1)\n",
        "\n",
        "    # Calculate comprehensive metrics\n",
        "    acc = accuracy_score(all_labels, preds)\n",
        "    bal_acc = balanced_accuracy_score(all_labels, preds)\n",
        "    precision = precision_score(all_labels, preds, average='weighted', zero_division=0)\n",
        "    recall = recall_score(all_labels, preds, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(all_labels, preds, average='weighted', zero_division=0)\n",
        "    cm = confusion_matrix(all_labels, preds)\n",
        "\n",
        "    results = {\n",
        "        'loss': total_loss / len(loader),\n",
        "        'accuracy': acc,\n",
        "        'balanced_accuracy': bal_acc,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'confusion_matrix': cm,\n",
        "        'threshold': thr_opt\n",
        "    }\n",
        "\n",
        "    if return_probs:\n",
        "        results['probabilities'] = mci_probs\n",
        "        results['labels'] = all_labels\n",
        "\n",
        "    return results\n",
        "\n",
        "# ==================== CHECKPOINTS ====================\n",
        "def save_checkpoint(model, optimizer, scheduler, epoch, metrics, path=\"best.pth\"):\n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'epoch': epoch,\n",
        "        'metrics': metrics\n",
        "    }\n",
        "    torch.save(checkpoint, path)\n",
        "\n",
        "def load_checkpoint(path=\"best.pth\", map_location=device):\n",
        "    if not os.path.exists(path):\n",
        "        return None\n",
        "    try:\n",
        "        return torch.load(path, map_location=map_location, weights_only=False)\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not load checkpoint '{path}': {e}\")\n",
        "        return None\n",
        "\n",
        "# ==================== MAIN ====================\n",
        "def main():\n",
        "    # Collect files\n",
        "    files, labels = [], []\n",
        "    for i, cls in enumerate(cfg.CLASSES):\n",
        "        d = os.path.join(cfg.DATA_ROOT, cls)\n",
        "        if not os.path.isdir(d):\n",
        "            continue\n",
        "        for f in os.listdir(d):\n",
        "            if f.endswith(('.nii', '.nii.gz')):\n",
        "                files.append(os.path.join(d, f))\n",
        "                labels.append(i)\n",
        "\n",
        "    files = np.array(files)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    print(f\"âœ“ Initially found {len(files)} files | CN={sum(labels==0)}, MCI={sum(labels==1)}\")\n",
        "\n",
        "    # RANDOM SAMPLING OF CN CASES\n",
        "    MAX_CN_SAMPLES = 440  # Cap number of CN samples to overcome huge class imbalance\n",
        "\n",
        "    if sum(labels==0) > MAX_CN_SAMPLES:\n",
        "        cn_indices = np.where(labels == 0)[0]\n",
        "        mci_indices = np.where(labels == 1)[0]\n",
        "\n",
        "        print(f\"ðŸ“Š Original CN samples: {len(cn_indices)}\")\n",
        "        print(f\"ðŸŽ¯ Randomly sampling {MAX_CN_SAMPLES} CN cases...\")\n",
        "\n",
        "        # Randomly sample CN cases\n",
        "        np.random.seed(42)  # For reproducibility\n",
        "        sampled_cn_indices = np.random.choice(cn_indices, size=MAX_CN_SAMPLES, replace=False)\n",
        "\n",
        "        # Combine sampled CN with all MCI\n",
        "        selected_indices = np.concatenate([sampled_cn_indices, mci_indices])\n",
        "\n",
        "        # Update files and labels\n",
        "        files = files[selected_indices]\n",
        "        labels = labels[selected_indices]\n",
        "\n",
        "        print(f\"âœ… Balanced dataset: CN={sum(labels==0)}, MCI={sum(labels==1)}\")\n",
        "    else:\n",
        "        print(f\"âœ“ Using all available samples: CN={sum(labels==0)}, MCI={sum(labels==1)}\")\n",
        "\n",
        "    # Check final class distribution\n",
        "    cn_count = sum(labels == 0)\n",
        "    mci_count = sum(labels == 1)\n",
        "    imbalance_ratio = cn_count / mci_count if mci_count > 0 else float('inf')\n",
        "    print(f\"ðŸ“Š Final class imbalance ratio: {imbalance_ratio:.2f}:1 (CN:MCI)\")\n",
        "\n",
        "    # Splits\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "        files, labels, test_size=0.15, stratify=labels, random_state=42\n",
        "    )\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_temp, y_temp, test_size=0.1765, stratify=y_temp, random_state=42\n",
        "    )\n",
        "\n",
        "    # Datasets\n",
        "    train_ds = OASISDataset(X_train, y_train, 'train')\n",
        "    val_ds = OASISDataset(X_val, y_val, 'val')\n",
        "    test_ds = OASISDataset(X_test, y_test, 'test')\n",
        "\n",
        "    # Calculate class weights properly\n",
        "    class_counts = np.bincount(y_train)\n",
        "    print(f\"âœ“ Training class distribution: CN={class_counts[0]}, MCI={class_counts[1]}\")\n",
        "\n",
        "    # Improved dynamic weighting\n",
        "    if cfg.USE_DYNAMIC_WEIGHTS:\n",
        "        # Calculate inverse frequency weights\n",
        "        total_samples = len(y_train)\n",
        "        weights = total_samples / (len(class_counts) * class_counts)\n",
        "        # Apply additional boost to minority class\n",
        "        weights[1] *= cfg.BASE_MCI_WEIGHT\n",
        "        print(f\"âœ“ Class weights: CN={weights[0]:.3f}, MCI={weights[1]:.3f}\")\n",
        "    else:\n",
        "        weights = np.array([1.0, cfg.BASE_MCI_WEIGHT])\n",
        "\n",
        "    # Create sample weights for balanced sampling\n",
        "    sample_weights = weights[y_train]\n",
        "    sampler = WeightedRandomSampler(\n",
        "        torch.from_numpy(sample_weights).double(),\n",
        "        len(sample_weights),\n",
        "        replacement=True\n",
        "    )\n",
        "\n",
        "    # Data loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_ds, batch_size=cfg.BATCH_SIZE, sampler=sampler,\n",
        "        num_workers=4, pin_memory=True, drop_last=True\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_ds, batch_size=cfg.BATCH_SIZE, shuffle=False,\n",
        "        num_workers=4, pin_memory=True\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_ds, batch_size=cfg.BATCH_SIZE, shuffle=False,\n",
        "        num_workers=4, pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Model, criterion, optimizer, scheduler\n",
        "    model = create_model(cfg.MODEL_TYPE)\n",
        "\n",
        "    # Use combined loss for better stability\n",
        "    criterion = CombinedLoss(focal_weight=0.7, ce_weight=0.3)\n",
        "\n",
        "    optimizer = optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=cfg.LEARNING_RATE,\n",
        "        weight_decay=cfg.WEIGHT_DECAY,\n",
        "        betas=(0.9, 0.999)\n",
        "    )\n",
        "\n",
        "    # Scheduler with warmup\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=cfg.LEARNING_RATE * 10,\n",
        "        epochs=cfg.NUM_EPOCHS,\n",
        "        steps_per_epoch=len(train_loader),\n",
        "        pct_start=0.1,  # 10% warmup\n",
        "        anneal_strategy='cos'\n",
        "    )\n",
        "\n",
        "    # Training loop with detailed monitoring\n",
        "    best_bal = -float(\"inf\")\n",
        "    patience = 0\n",
        "    best_metrics = {}\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"STARTING TRAINING WITH IMPROVED MONITORING\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    for epoch in range(cfg.NUM_EPOCHS):\n",
        "        # Unfreeze all layers after warmup\n",
        "        if epoch == cfg.WARMUP_EPOCHS:\n",
        "            for param in model.parameters():\n",
        "                param.requires_grad = True\n",
        "            print(f\"âœ“ Unfroze all layers at epoch {epoch+1}\")\n",
        "\n",
        "        # Training\n",
        "        tr_loss, tr_acc, tr_bal = train_epoch(model, train_loader, criterion, optimizer, epoch)\n",
        "\n",
        "        # Validation\n",
        "        val_results = evaluate(model, val_loader, criterion)\n",
        "\n",
        "        # Step scheduler\n",
        "        if isinstance(scheduler, optim.lr_scheduler.OneCycleLR):\n",
        "            # OneCycleLR steps per batch\n",
        "            pass\n",
        "        else:\n",
        "            scheduler.step()\n",
        "\n",
        "        # Print detailed metrics\n",
        "        print(f\"\\nEpoch {epoch+1}/{cfg.NUM_EPOCHS}:\")\n",
        "        print(f\"  Train - Loss: {tr_loss:.4f}, Acc: {tr_acc:.3f}, Bal: {tr_bal:.3f}\")\n",
        "        print(f\"  Val   - Loss: {val_results['loss']:.4f}, Acc: {val_results['accuracy']:.3f}, Bal: {val_results['balanced_accuracy']:.3f}\")\n",
        "        print(f\"  Val   - Precision: {val_results['precision']:.3f}, Recall: {val_results['recall']:.3f}, F1: {val_results['f1']:.3f}\")\n",
        "        print(f\"  Threshold: {val_results['threshold']:.3f}, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "        # Check for improvement\n",
        "        current_bal = val_results['balanced_accuracy']\n",
        "        if current_bal > best_bal + cfg.MIN_DELTA:\n",
        "            best_bal = current_bal\n",
        "            best_metrics = val_results.copy()\n",
        "            patience = 0\n",
        "\n",
        "            # Save checkpoint\n",
        "            save_checkpoint(model, optimizer, scheduler, epoch, val_results, 'best.pth')\n",
        "            print(f\"  âœ… New best balanced accuracy: {current_bal:.3f}\")\n",
        "        else:\n",
        "            patience += 1\n",
        "            print(f\"  â³ No improvement. Patience: {patience}/{cfg.PATIENCE}\")\n",
        "\n",
        "            if patience >= cfg.PATIENCE:\n",
        "                print(f\"  ðŸ›‘ Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    # Load best model\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"LOADING BEST MODEL FOR FINAL EVALUATION\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    ckpt = load_checkpoint('best.pth', map_location=device)\n",
        "    if ckpt is not None and 'model_state_dict' in ckpt:\n",
        "        model.load_state_dict(ckpt['model_state_dict'])\n",
        "        print(f\"âœ“ Loaded best model from epoch {ckpt['epoch']+1}\")\n",
        "        print(f\"âœ“ Best validation balanced accuracy: {ckpt['metrics']['balanced_accuracy']:.3f}\")\n",
        "    else:\n",
        "        print(\"âš  Warning: Using current model (no checkpoint loaded)\")\n",
        "\n",
        "    # Final test evaluation\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"FINAL TEST EVALUATION\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    test_results = evaluate(model, test_loader, criterion, return_probs=True)\n",
        "\n",
        "    # Print comprehensive results\n",
        "    print(f\"\\nðŸŽ¯ FINAL TEST RESULTS:\")\n",
        "    print(f\"  Accuracy: {test_results['accuracy']:.3f} ({test_results['accuracy']*100:.1f}%)\")\n",
        "    print(f\"  Balanced Accuracy: {test_results['balanced_accuracy']:.3f} ({test_results['balanced_accuracy']*100:.1f}%)\")\n",
        "    print(f\"  Precision: {test_results['precision']:.3f}\")\n",
        "    print(f\"  Recall: {test_results['recall']:.3f}\")\n",
        "    print(f\"  F1-Score: {test_results['f1']:.3f}\")\n",
        "    print(f\"  Optimal Threshold: {test_results['threshold']:.3f}\")\n",
        "\n",
        "    # Detailed confusion matrix analysis\n",
        "    cm = test_results['confusion_matrix']\n",
        "    if cm.shape == (2, 2):\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "        ppv = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
        "\n",
        "        print(f\"\\nðŸ“Š MEDICAL METRICS:\")\n",
        "        print(f\"  Sensitivity (MCI Detection): {sensitivity:.3f} ({sensitivity*100:.1f}%)\")\n",
        "        print(f\"  Specificity (CN Detection): {specificity:.3f} ({specificity*100:.1f}%)\")\n",
        "        print(f\"  Positive Predictive Value: {ppv:.3f} ({ppv*100:.1f}%)\")\n",
        "        print(f\"  Negative Predictive Value: {npv:.3f} ({npv*100:.1f}%)\")\n",
        "\n",
        "    print(f\"\\nðŸ“ˆ CONFUSION MATRIX:\")\n",
        "    print(\"     Predicted\")\n",
        "    print(\"      CN  MCI\")\n",
        "    print(f\" CN  {cm[0,0]:3d} {cm[0,1]:3d}\")\n",
        "    print(f\"MCI  {cm[1,0]:3d} {cm[1,1]:3d}\")\n",
        "\n",
        "    # Calculate AUC\n",
        "    if 'probabilities' in test_results and len(np.unique(test_results['labels'])) > 1:\n",
        "        try:\n",
        "            from sklearn.metrics import roc_auc_score\n",
        "            test_auc = roc_auc_score(test_results['labels'], test_results['probabilities'])\n",
        "            print(f\"\\nðŸŽ¯ AUC-ROC: {test_auc:.3f}\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    print(f\"\\nâœ… TRAINING COMPLETED SUCCESSFULLY!\")\n",
        "    print(f\"ðŸ“„ Best model saved as 'best.pth'\")\n",
        "\n",
        "    return test_results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}