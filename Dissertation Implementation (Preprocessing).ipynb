{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nibabel as nib\n",
        "import numpy as np\n",
        "import os\n",
        "from scipy.ndimage import zoom\n",
        "from pathlib import Path\n",
        "\n",
        "# === CONFIG ===\n",
        "input_root = Path(\"D:/Dissertation/NIfTI_Output\")\n",
        "output_root = Path(\"D:/Dissertation/Preprocessed\")\n",
        "target_shape = (128, 128, 128)\n",
        "\n",
        "# === FUNCTIONS ===\n",
        "def normalize(volume):\n",
        "    volume = volume.astype(np.float32)\n",
        "    volume = (volume - np.mean(volume)) / np.std(volume)\n",
        "    return volume\n",
        "\n",
        "def resize_volume(volume, target_shape):\n",
        "    current_shape = volume.shape\n",
        "    zoom_factors = [t / c for t, c in zip(target_shape, current_shape)]\n",
        "    return zoom(volume, zoom_factors, order=1)  # order=1 = linear interpolation\n",
        "\n",
        "def preprocess_and_save(nii_path, output_path):\n",
        "    img = nib.load(str(nii_path))\n",
        "    volume = img.get_fdata()\n",
        "    volume = normalize(volume)\n",
        "    volume = resize_volume(volume, target_shape)\n",
        "\n",
        "    new_img = nib.Nifti1Image(volume, np.eye(4))\n",
        "    nib.save(new_img, str(output_path))\n",
        "\n",
        "# === PROCESSING LOOP ===\n",
        "for label in [\"CN\", \"MCI\", \"AD\"]:\n",
        "    input_folder = input_root / label\n",
        "    output_folder = output_root / label\n",
        "    output_folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for nii_file in input_folder.glob(\"*.nii.gz\"):\n",
        "        subject_id = nii_file.stem\n",
        "        output_path = output_folder / f\"{subject_id}.nii.gz\"\n",
        "\n",
        "        try:\n",
        "            preprocess_and_save(nii_file, output_path)\n",
        "            print(f\"âœ… Processed: {subject_id}\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error processing {subject_id}: {e}\")\n"
      ],
      "metadata": {
        "id": "q--0PaKDNPXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C94U_zaTO8aO",
        "outputId": "8403c8dc-68e3-4112-f3b5-77115b6f95dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m52.6/52.6 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m113.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for hd-bet (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for nnunetv2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for acvl-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for batchgenerators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for batchgeneratorsv2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for dynamic-network-architectures (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Mounted at /content/drive\n",
            "HD-BET device: cuda\n",
            "Found 297 AD files to check/convert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Skull stripping AD: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 297/297 [4:16:28<00:00, 51.81s/file]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== SUMMARY (AD) ===\n",
            "Total in source     : 297\n",
            "Converted new files : 297\n",
            "Skipped (already _SS): 0\n",
            "Deleted mask files  : 0\n",
            "Failed conversions  : 0\n",
            "Output folder: /content/drive/MyDrive/OASIS3_Preprocessed_Data/MCI_New\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ==================\n",
        "# Skull-stripping\n",
        "# ==================\n",
        "!pip -q install nibabel tqdm\n",
        "try:\n",
        "    import HD_BET\n",
        "except Exception:\n",
        "    !pip -q install hd-bet\n",
        "\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import re\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "# ---- Paths ----\n",
        "SRC_DIR = Path(\"/content/drive/MyDrive/OASIS_New/MCI\")  # source\n",
        "DST_DIR = Path(\"/content/drive/MyDrive/OASIS3_Preprocessed_Data/MCI_New\")  # destination (only *_SS.nii.gz brains)\n",
        "DST_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"HD-BET device:\", DEVICE)\n",
        "\n",
        "def is_nii(p: Path) -> bool:\n",
        "    n = p.name.lower()\n",
        "    return n.endswith(\".nii\") or n.endswith(\".nii.gz\")\n",
        "\n",
        "def ensure_ss_name(src_name: str) -> str:\n",
        "    \"\"\"Make a single output name with _SS and .nii.gz extension.\"\"\"\n",
        "    if src_name.lower().endswith(\".nii.gz\"):\n",
        "        stem, ext = src_name[:-7], \".nii.gz\"\n",
        "    elif src_name.lower().endswith(\".nii\"):\n",
        "        stem, ext = src_name[:-4], \".nii.gz\"\n",
        "    else:\n",
        "        stem, ext = Path(src_name).stem, \".nii.gz\"\n",
        "    if not stem.endswith(\"_SS\"):\n",
        "        stem += \"_SS\"\n",
        "    return f\"{stem}{ext}\"\n",
        "\n",
        "def run_hdbet(in_file: Path, out_file: Path) -> bool:\n",
        "    \"\"\"\n",
        "    Call hd-bet to create ONE output: the skull-stripped image at out_file.\n",
        "    We DO NOT request the mask to avoid *_SS_bet duplicates.\n",
        "    \"\"\"\n",
        "    cmd = [\n",
        "        \"hd-bet\",\n",
        "        \"-i\", str(in_file),\n",
        "        \"-o\", str(out_file),\n",
        "        \"-device\", DEVICE,\n",
        "        # DO NOT USE: \"--save_bet_mask\"\n",
        "\n",
        "    ]\n",
        "    res = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "    if res.returncode != 0:\n",
        "        last = (res.stderr or res.stdout).splitlines()[-1] if (res.stderr or res.stdout) else \"unknown error\"\n",
        "        print(f\"  âŒ HD-BET failed: {in_file.name} -> {last}\")\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "# ---------- CLEANUP HELPERS ----------\n",
        "MASK_SUFFIX_PATTERNS = [\n",
        "    re.compile(r\"_SS_bet\\.nii\\.gz$\", re.IGNORECASE),\n",
        "    re.compile(r\"_bet\\.nii\\.gz$\", re.IGNORECASE),\n",
        "    re.compile(r\"_mask\\.nii\\.gz$\", re.IGNORECASE),\n",
        "]\n",
        "\n",
        "def looks_like_mask(name: str) -> bool:\n",
        "    lower = name.lower()\n",
        "    return any(p.search(lower) for p in MASK_SUFFIX_PATTERNS)\n",
        "\n",
        "def cleanup_masks(dst_dir: Path):\n",
        "    \"\"\"\n",
        "    Remove any lingering mask files like *_SS_bet.nii.gz, *_bet.nii.gz, *_mask.nii.gz\n",
        "    IF the paired *_SS.nii.gz brain image exists. Otherwise leave them alone.\n",
        "    \"\"\"\n",
        "    deleted = 0\n",
        "    for p in sorted(dst_dir.glob(\"*.nii*\")):\n",
        "        if not p.is_file():\n",
        "            continue\n",
        "        if looks_like_mask(p.name):\n",
        "            base = p.name\n",
        "            base = re.sub(r\"_SS_bet\\.nii\\.gz$\", \"_SS.nii.gz\", base, flags=re.IGNORECASE)\n",
        "            base = re.sub(r\"_bet\\.nii\\.gz$\", \"_SS.nii.gz\",   base, flags=re.IGNORECASE)\n",
        "            base = re.sub(r\"_mask\\.nii\\.gz$\", \"_SS.nii.gz\",  base, flags=re.IGNORECASE)\n",
        "            paired = dst_dir / base\n",
        "            if paired.exists():\n",
        "                try:\n",
        "                    p.unlink()\n",
        "                    deleted += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"  âš ï¸ Could not delete {p.name}: {e}\")\n",
        "    return deleted\n",
        "\n",
        "# ---------- MAIN ----------\n",
        "# 1) Clean any old mask files (if a good *_SS exists)\n",
        "deleted_masks = cleanup_masks(DST_DIR)\n",
        "if deleted_masks:\n",
        "    print(f\"ğŸ§¹ Removed {deleted_masks} stray mask files from {DST_DIR}.\")\n",
        "\n",
        "# 2) Process sources\n",
        "src_files = [p for p in sorted(SRC_DIR.rglob(\"*\")) if p.is_file() and is_nii(p)]\n",
        "print(f\"Found {len(src_files)} AD files to check/convert\")\n",
        "\n",
        "converted, skipped, failed = 0, 0, []\n",
        "\n",
        "for p in tqdm(src_files, desc=\"Skull stripping AD\", unit=\"file\"):\n",
        "    out_name = ensure_ss_name(p.name)\n",
        "    out_path = DST_DIR / out_name\n",
        "\n",
        "    # If the desired brain-extracted file already exists, just skip\n",
        "    if out_path.exists():\n",
        "        skipped += 1\n",
        "        continue\n",
        "\n",
        "    # Otherwise run hd-bet to create the _SS file ONLY\n",
        "    if run_hdbet(p, out_path):\n",
        "        converted += 1\n",
        "    else:\n",
        "        failed.append(str(p))\n",
        "\n",
        "# 3) Final cleanup pass (just in case)\n",
        "deleted_masks_2 = cleanup_masks(DST_DIR)\n",
        "deleted_masks += deleted_masks_2\n",
        "if deleted_masks_2:\n",
        "    print(f\"ğŸ§¹ Removed {deleted_masks_2} more stray mask files.\")\n",
        "\n",
        "print(\"\\n=== SUMMARY (AD) ===\")\n",
        "print(f\"Total in source     : {len(src_files)}\")\n",
        "print(f\"Converted new files : {converted}\")\n",
        "print(f\"Skipped (already _SS): {skipped}\")\n",
        "print(f\"Deleted mask files  : {deleted_masks}\")\n",
        "print(f\"Failed conversions  : {len(failed)}\")\n",
        "if failed:\n",
        "    print(\"First few failures:\")\n",
        "    for f in failed[:10]:\n",
        "        print(\"  -\", f)\n",
        "print(\"Output folder:\", DST_DIR)\n"
      ]
    }
  ]
}